{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9816e3ee-dc07-499a-b1ed-f6f607e5c2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe9797c8-a6f8-41de-916b-62db79e7c3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_or_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "lora_path = \"./III_Finetuning_For_RAG/mistral-7b-int4-dolly/checkpoint-82\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c32cedc-136a-428f-8b7c-6378aa7b51e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perplexity(nn.Module):\n",
    "    def __init__(self, reduce: bool = True):\n",
    "        super().__init__()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        \"\"\"\n",
    "        logits: LLM的原始输出，尚未应用softmax的概率分布. shape：(样本数，token数，词表大小)\n",
    "        labels: 正确的token索引. shape: (样本数，token数）\n",
    "        \"\"\"\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "        perplexity = []\n",
    "        for i in range(labels.shape[0]):\n",
    "            perplexity.append(self.loss_fn(shift_logits[i], shift_labels[i]))\n",
    "        perplexity = torch.stack(perplexity, dim=0)\n",
    "        #perplexity = torch.exp(perplexity)\n",
    "        if self.reduce:\n",
    "            perplexity = torch.mean(perplexity)\n",
    "        return perplexity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17c61579-8807-4ce9-9b5e-a90b5103345e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0050814151763916016,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 41,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0126a86913514c60a17fd7c72434b6f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_model_and_tokenizer(model_path_or_id, lora_path=None):\n",
    "    if lora_path:\n",
    "        # load base LLM model with PEFT Adapter\n",
    "        model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "            lora_path,\n",
    "            low_cpu_mem_usage=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            use_flash_attention_2=True,\n",
    "            load_in_4bit=True,\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(lora_path)\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path_or_id,\n",
    "            low_cpu_mem_usage=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            use_flash_attention_2=True,\n",
    "            load_in_4bit=True\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path_or_id)\n",
    "        \n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(\"mistralai/Mistral-7B-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "897228e0-556a-4518-88cc-6bb788962ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  3.307\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I have a good idea.\"\n",
    "ppl = Perplexity(reduce=True)\n",
    "\n",
    "def calculate_perplexity_for_sentence(model, tokenizer, ppl, sentence):\n",
    "    '''\n",
    "    对给定的句子或句子列表，和指定的LLM，计算perplexity\n",
    "    '''\n",
    "    with torch.inference_mode():\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        inputs = tokenizer(\n",
    "            sentence, return_tensors=\"pt\", padding=True, truncation=True\n",
    "        ).to(\"cuda\")\n",
    "        output = model(\n",
    "            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n",
    "        )\n",
    "        logits = output.logits\n",
    "        labels = inputs[\"input_ids\"]\n",
    "        labels.masked_fill_(~inputs[\"attention_mask\"].bool(), -100)\n",
    "    perplexity = ppl(logits, labels).detach().cpu().numpy()\n",
    "    return perplexity\n",
    "\n",
    "perplexity = calculate_perplexity_for_sentence(model, tokenizer, ppl, sentence)\n",
    "print(f\"Perplexity: {perplexity: .3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "734d22eb-911e-4137-9ebb-1772f361b259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003503084182739258,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 41,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d6ca9fd512e47cf9eb0c321c544b9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  4.093\n"
     ]
    }
   ],
   "source": [
    "ft_model, ft_tokenizer = load_model_and_tokenizer(None, lora_path=lora_path)\n",
    "perplexity = calculate_perplexity_for_sentence(ft_model, ft_tokenizer, ppl, sentence)\n",
    "print(f\"Perplexity: {perplexity: .3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30080d9d-185b-436e-9e26-e88b177e8608",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
