{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7496f4ab-96d6-47a0-9cbe-ec05a905b54c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -U deepeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62dc543e-c9ea-4639-b0ef-a70b7dcad249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export OPENAI_API_KEY=<your api key>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb2840f7-9d57-4daf-a7bf-9170efd8c776",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !deepeval login --confident-api-key <your_api_key>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e85dcd9-5fe5-493c-b566-41baded8f5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: line 0: syntax error near unexpected token `newline'\n",
      "/bin/bash: -c: line 0: `huggingface-cli login --token <your_token>'\n"
     ]
    }
   ],
   "source": [
    "# !huggingface-cli login --token <your_token>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c8a591a-c632-4789-adea-432163ebc7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005366086959838867,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 41,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c59d250abbc34b6b82d9332dfeb41a64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# model_path_or_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "model_path_or_id = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "lora_path = None\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "if lora_path:\n",
    "    # load base LLM model with PEFT Adapter\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        lora_path,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        use_flash_attention_2=True,\n",
    "        quantization_config = bnb_config\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(lora_path)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path_or_id,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        use_flash_attention_2=True,\n",
    "        quantization_config = bnb_config\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path_or_id)\n",
    "\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_path_or_id,\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     use_flash_attention_2=True\n",
    "# ).cuda()\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path_or_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32c44f64-94f5-47e8-b5a0-93b4481a466c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> Write me a joke about a chicken\\n\\nHere's a joke about a chicken:\\n\\nWhy did the chicken go to the doctor?\\n\\nBecause she had fowl breath!\\n\\nI hope you found that joke to be egg-cellent!</s>\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "\n",
    "class Llama13B(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        model = self.load_model()\n",
    "        input_ids = self.tokenizer([prompt], return_tensors=\"pt\").input_ids.cuda()\n",
    "        generated_ids = model.generate(input_ids=input_ids, max_new_tokens=1024, do_sample=True)\n",
    "        return self.tokenizer.batch_decode(generated_ids)[0]\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        return self.generate(prompt)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"LLama 2 13B\"\n",
    "\n",
    "judger = Llama13B(model=model, tokenizer=tokenizer)\n",
    "judger.generate(\"Write me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde5f7be-41b0-45bf-adde-cca8f65ad25e",
   "metadata": {},
   "source": [
    "### Answer Relevancy Metric\n",
    "The AnswerRelevancyMetric score is calculated according to the following equation:\n",
    "\n",
    "***Answer Relevancy = Number of Relevant Statements / Total Number of Statements***\n",
    "\n",
    "The AnswerRelevancyMetric first uses an LLM to extract all statements made in the actual_output, before using the same LLM to classify whether each statement is relevant to the input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "421a045a-ce21-4e00-ab2a-bb7cefddb846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0c7af94ffe44fa8d30b387cadc4153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "<s> Given the answer relevancy score, the list of reasons of irrelevant statements made in the actual output, and the input, provide a CONCISE reason for the score. Explain why it is not higher, but also why it is at its current score.\n",
      "The irrelevant statements represent things in the actual output that is irrelevant to addressing whatever is asked/talked about in the input.\n",
      "If there is nothing irrelevant, just say something positive with an upbeat encouraging tone (but don't overdo it otherwise it gets annoying).\n",
      "\n",
      "Answer Relevancy Score:\n",
      "0.75\n",
      "\n",
      "Reasons why the score can't be higher based on irrelevant statements in the actual output:\n",
      "[\"The 'Shoes.' statement made in the actual output is completely irrelevant to the input, which asks about what to do in the event of an earthquake.\"]\n",
      "\n",
      "Input:\n",
      "What if these shoes don't fit?\n",
      "\n",
      "Example:\n",
      "The score is <answer_relevancy_score> because <your_reason>.\n",
      "\n",
      "Reason:\n",
      "The irrelevant statement \"The 'Shoes.' statement made in the actual output is completely irrelevant to the input, which asks about what to do in the event of an earthquake.\"</s>\n",
      "Evaluating test cases...\n",
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 0.75, threshold: 0.5, strict: False, evaluation model: LLama 2 13B, reason: <s> Given the answer relevancy score, the list of reasons of irrelevant statements made in the actual output, and the input, provide a CONCISE reason for the score. Explain why it is not higher, but also why it is at its current score.\n",
      "The irrelevant statements represent things in the actual output that is irrelevant to addressing whatever is asked/talked about in the input.\n",
      "If there is nothing irrelevant, just say something positive with an upbeat encouraging tone (but don't overdo it otherwise it gets annoying).\n",
      "\n",
      "Answer Relevancy Score:\n",
      "0.75\n",
      "\n",
      "Reasons why the score can't be higher based on irrelevant statements in the actual output:\n",
      "[\"The 'Shoes.' statement made in the actual output is completely irrelevant to the input, which asks about what to do in the event of an earthquake.\"]\n",
      "\n",
      "Input:\n",
      "What if these shoes don't fit?\n",
      "\n",
      "Example:\n",
      "The score is <answer_relevancy_score> because <your_reason>.\n",
      "\n",
      "Reason:\n",
      "The answer is mostly relevant, but the statement \"The 'Shoes.' statement made in the actual output is completely irrelevant to the input, which asks about what to do in the event of an earthquake.\"\n",
      "\n",
      "Explanation:\n",
      "The answer is mostly relevant to the input, but the statement \"The 'Shoes.' statement made in the actual output is completely irrelevant to the input, which asks about what to do in the event of an earthquake.\" brings down the overall relevancy score. The statement is not related to the topic of earthquakes or what to do during one, and therefore detracts from the overall relevancy of the answer.</s>)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What if these shoes don't fit?\n",
      "  - actual output: We offer a 30-day full refund at no extra cost.\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Tests finished! Run <span style=\"color: #008000; text-decoration-color: #008000\">\"deepeval login\"</span> to view evaluation results on the web.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Tests finished! Run \u001b[32m\"deepeval login\"\u001b[0m to view evaluation results on the web.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[TestResult(success=True, metrics=[<deepeval.metrics.answer_relevancy.answer_relevancy.AnswerRelevancyMetric object at 0x7fcf580b9400>], input=\"What if these shoes don't fit?\", actual_output='We offer a 30-day full refund at no extra cost.', expected_output=None, context=None, retrieval_context=None)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Replace this with the actual output from your LLM application\n",
    "actual_output = \"We offer a 30-day full refund at no extra cost.\"\n",
    "\n",
    "metric = AnswerRelevancyMetric(\n",
    "    threshold=0.5,\n",
    "    model=judger,\n",
    "    include_reason=True\n",
    ")\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What if these shoes don't fit?\",\n",
    "    actual_output=actual_output\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "# or evaluate test cases in bulk\n",
    "evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05a6dce2-23fb-46c5-b825-c95b058b31ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "print(metric.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abea658-6964-412d-bc68-02c849d98677",
   "metadata": {},
   "source": [
    "### Hallucination Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232bb120-0bbe-4285-92a0-7f7417e097ac",
   "metadata": {},
   "source": [
    "The HallucinationMetric score is calculated according to the following equation:\n",
    "\n",
    "***Hallucination = Number of Contradicted Contexts / Total Number of Contexts***\n",
    " \n",
    "The HallucinationMetric uses an LLM to determine, for each context in contexts, whether there are any contradictions to the actual_output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df41f541-347e-43b1-8850-3db0cd57b5a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "<s> Given a list of factual alignments and contradictions, which highlights alignment/contradictions between the `actual output` and `contexts, use it to provide a reason for the hallucination score in a CONCISELY. Note that The hallucination score ranges from 0 - 1, and the lower the better.\n",
      "\n",
      "Factual Alignments:\n",
      "['The actual output contradicts the provided context which states that Einstein won the Nobel Prize in 1968, not 1969.']\n",
      "\n",
      "Contradictions:\n",
      "['The actual output agrees with the provided context which states that Einstein won the Nobel Prize for his discovery of the photoelectric effect.']\n",
      "\n",
      "Hallucination Score:\n",
      "0.50\n",
      "\n",
      "Example:\n",
      "The score is <hallucination_score> because <your_reason>.\n",
      "\n",
      "Reason:\n",
      "The hallucination score is 0.50 because there is a factual alignment between the actual output and the context that states Einstein won the Nobel Prize for his discovery of the photoelectric effect, but a contradiction in the actual output regarding the year he won the prize.</s>\n"
     ]
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import HallucinationMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Replace this with the actual documents that you are passing as input to your LLM.\n",
    "context=[\"A man with blond-hair and a brown shirt drinking out of a public water fountain.\"]\n",
    "\n",
    "# Replace this with the actual output from your LLM application\n",
    "actual_output=\"A blond drinking water in public.\"\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What was the blond doing?\",\n",
    "    actual_output=actual_output,\n",
    "    context=context\n",
    ")\n",
    "metric = HallucinationMetric(\n",
    "    threshold=0.5,\n",
    "    model=judger\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2931e76-cddb-4232-b645-9bfe4adad3ff",
   "metadata": {},
   "source": [
    "### Contextual Relevancy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b3eb16-4982-469f-8d08-74e84dff18cb",
   "metadata": {},
   "source": [
    "The ContextualRelevancyMetric score is calculated according to the following equation:\n",
    "\n",
    "***Contextual Relevancy = Number of Relevant Statements / Total Number of Statements***\n",
    " \n",
    "Although similar to how the AnswerRelevancyMetric is calculated, the ContextualRelevancyMetric first uses an LLM to extract all statements made in the retrieval_context instead, before using the same LLM to classify whether each statement is relevant to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f3e1be5-43ff-49e5-bf9f-8da147d0d600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c9add1423364fca98e493fdab87d505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "<s> Based on the given input, reasons for why the retrieval context is irrelevant to the input, and the contextual relevancy score (the closer to 1 the better), please generate a CONCISE reason for the score.\n",
      "In your reason, you should quote data provided in the reasons for irrelevancy to support your point.\n",
      "\n",
      "Contextual Relevancy Score:\n",
      "0.00\n",
      "\n",
      "Input:\n",
      "What if these shoes don't fit?\n",
      "\n",
      "Reasons for why the retrieval context is irrelevant to the input:\n",
      "[None]\n",
      "\n",
      "Example:\n",
      "The score is <contextual_relevancy_score> because <your_reason>.\n",
      "\n",
      "** \n",
      "IMPORTANT:\n",
      "If the score is 1, keep it short and say something positive with an upbeat encouraging tone (but don't overdo it otherwise it gets annoying).\n",
      "**\n",
      "\n",
      "Reason:\n",
      "The contextual relevancy score is 0.00 because there are no reasons provided for why the retrieval context is irrelevant to the input.</s>\n",
      "Evaluating test cases...\n",
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ❌ Contextual Relevancy (score: 0.0, threshold: 0.5, strict: False, evaluation model: LLama 2 13B, reason: <s> Based on the given input, reasons for why the retrieval context is irrelevant to the input, and the contextual relevancy score (the closer to 1 the better), please generate a CONCISE reason for the score.\n",
      "In your reason, you should quote data provided in the reasons for irrelevancy to support your point.\n",
      "\n",
      "Contextual Relevancy Score:\n",
      "0.00\n",
      "\n",
      "Input:\n",
      "What if these shoes don't fit?\n",
      "\n",
      "Reasons for why the retrieval context is irrelevant to the input:\n",
      "[None]\n",
      "\n",
      "Example:\n",
      "The score is <contextual_relevancy_score> because <your_reason>.\n",
      "\n",
      "** \n",
      "IMPORTANT:\n",
      "If the score is 1, keep it short and say something positive with an upbeat encouraging tone (but don't overdo it otherwise it gets annoying).\n",
      "**\n",
      "\n",
      "Reason:\n",
      "The contextual relevancy score is 0.00 because there are no reasons provided for why the retrieval context is irrelevant to the input.</s>)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What if these shoes don't fit?\n",
      "  - actual output: We offer a 30-day full refund at no extra cost.\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: ['All customers are eligible for a 30 day full refund at no extra cost.']\n",
      "\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Tests finished! Run <span style=\"color: #008000; text-decoration-color: #008000\">\"deepeval login\"</span> to view evaluation results on the web.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Tests finished! Run \u001b[32m\"deepeval login\"\u001b[0m to view evaluation results on the web.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[TestResult(success=False, metrics=[<deepeval.metrics.contextual_relevancy.contextual_relevancy.ContextualRelevancyMetric object at 0x7fcf2ba164c0>], input=\"What if these shoes don't fit?\", actual_output='We offer a 30-day full refund at no extra cost.', expected_output=None, context=None, retrieval_context=['All customers are eligible for a 30 day full refund at no extra cost.'])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import ContextualRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Replace this with the actual output from your LLM application\n",
    "actual_output = \"We offer a 30-day full refund at no extra cost.\"\n",
    "\n",
    "# Replace this with the actual retrieved context from your RAG pipeline\n",
    "retrieval_context = [\"All customers are eligible for a 30 day full refund at no extra cost.\"]\n",
    "\n",
    "metric = ContextualRelevancyMetric(\n",
    "    threshold=0.5,\n",
    "    model=judger,\n",
    "    include_reason=True\n",
    ")\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What if these shoes don't fit?\",\n",
    "    actual_output=actual_output,\n",
    "    retrieval_context=retrieval_context\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "# or evaluate test cases in bulk\n",
    "evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3543e9c-f7fc-4c39-b1c5-09ef251a3f03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
